{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7752b192",
   "metadata": {},
   "source": [
    "# Multi-Audio Speaker Identification & Analysis\n",
    "\n",
    "This notebook processes multiple audio files to:\n",
    "1. **Transcribe and diarize** speakers in each audio file\n",
    "2. **Identify speakers** using AI-powered analysis\n",
    "3. **Compare speaker patterns** across different audio files\n",
    "4. **Extract topics** and speaking characteristics per speaker\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56bdcf5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\norak\\SpeakSense\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# IMPORTS AND SETUP\n",
    "# ============================================================\n",
    "import whisperx\n",
    "import gc\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Audio processing\n",
    "import librosa\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# AI and NLP\n",
    "from openai import OpenAI\n",
    "from keybert import KeyBERT\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Visualization\n",
    "from wordcloud import WordCloud\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69c0938b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Audio files found:\n",
      "  YES US_Debate: US_DebateAudio.wav\n",
      "  YES Irish_Presidential: IrishPresidentalElection.wav\n",
      "  YES Sample_Audio: sampleaudio.wav\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CONFIGURATION AND HELPER FUNCTIONS\n",
    "# ============================================================\n",
    "\n",
    "# Audio files to process\n",
    "DATA_DIR = Path(\"../data\")\n",
    "AUDIO_FILES = {\n",
    "    \"US_Debate\": DATA_DIR / \"US_DebateAudio.wav\",\n",
    "    \"Irish_Presidential\": DATA_DIR / \"IrishPresidentalElection.wav\", \n",
    "    \"Sample_Audio\": DATA_DIR / \"sampleaudio.wav\"\n",
    "}\n",
    "\n",
    "# Device setup\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "batch_size = 16\n",
    "compute_type = \"float16\" if device == \"cuda\" else \"int8\"\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"Audio files found:\")\n",
    "for name, path in AUDIO_FILES.items():\n",
    "    exists = \"YES\" if path.exists() else \"NO\"\n",
    "    print(f\"  {exists} {name}: {path.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "333a3b53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helper functions defined successfully!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CONTEXT DETECTION AND HELPER FUNCTIONS\n",
    "# ============================================================\n",
    "\n",
    "def detect_context(full_text):\n",
    "    \"\"\"Detect the type of conversation from transcript text.\"\"\"\n",
    "    text = full_text.lower()\n",
    "    \n",
    "    # Political debates/discussions\n",
    "    if any(w in text for w in [\"president\", \"administration\", \"policy\", \"senator\", \"congress\", \"election\"]):\n",
    "        if \"irish\" in text or \"ireland\" in text:\n",
    "            return \"Irish political debate/election\"\n",
    "        return \"US political debate\"\n",
    "    \n",
    "    # Economic discussions\n",
    "    if any(w in text for w in [\"budget\", \"inflation\", \"economy\", \"financial\", \"market\"]):\n",
    "        return \"Economic/financial discussion\"\n",
    "    \n",
    "    # Interviews/podcasts\n",
    "    if any(w in text for w in [\"host\", \"welcome\", \"interview\", \"guest\", \"today we have\"]):\n",
    "        return \"Interview/podcast\"\n",
    "    \n",
    "    # Educational/academic\n",
    "    if any(w in text for w in [\"research\", \"study\", \"analysis\", \"data\", \"methodology\"]):\n",
    "        return \"Academic/educational discussion\"\n",
    "    \n",
    "    return \"General conversation\"\n",
    "\n",
    "def chunk_text(text, max_len=1200):\n",
    "    \"\"\"Limit text length for AI processing efficiency.\"\"\"\n",
    "    return text[:max_len]\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean transcript text by removing filler words and normalizing.\"\"\"\n",
    "    import re\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove common filler words\n",
    "    filler_words = [\"um\", \"uh\", \"like\", \"you know\", \"yeah\", \"ok\", \"right\", \"well\"]\n",
    "    for fw in filler_words:\n",
    "        text = re.sub(rf'\\b{re.escape(fw)}\\b', '', text)\n",
    "    \n",
    "    # Remove extra spaces and normalize\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "print(\"Helper functions defined successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae70ecee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading WhisperX models...\n",
      "2025-11-24 20:01:44 - whisperx.asr - INFO - No language specified, language will be detected for each audio file (increases inference time)\n",
      "2025-11-24 20:01:44 - whisperx.vads.pyannote - INFO - Performing voice activity detection using Pyannote...\n",
      "2025-11-24 20:01:44 - whisperx.asr - INFO - No language specified, language will be detected for each audio file (increases inference time)\n",
      "2025-11-24 20:01:44 - whisperx.vads.pyannote - INFO - Performing voice activity detection using Pyannote...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.5.5. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint c:\\Users\\norak\\SpeakSense\\venv\\Lib\\site-packages\\whisperx\\assets\\pytorch_model.bin`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model was trained with pyannote.audio 0.0.1, yours is 3.4.0. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
      "Model was trained with torch 1.10.0+cu102, yours is 2.8.0+cpu. Bad things might happen unless you revert torch to 1.x.\n",
      "ASR model loaded\n",
      "ASR model loaded\n",
      "Alignment model loaded\n",
      "Alignment model loaded\n",
      "Diarization model loaded (pyannote)\n",
      "All WhisperX models ready!\n",
      "Diarization model loaded (pyannote)\n",
      "All WhisperX models ready!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# WHISPERX SETUP AND AUDIO PROCESSING\n",
    "# ============================================================\n",
    "\n",
    "# Initialize WhisperX models\n",
    "print(\"Loading WhisperX models...\")\n",
    "\n",
    "# Load ASR model\n",
    "model = whisperx.load_model(\"large-v2\", device, compute_type=compute_type)\n",
    "print(\"ASR model loaded\")\n",
    "\n",
    "# Load alignment model\n",
    "model_a, metadata = whisperx.load_align_model(language_code=\"en\", device=device)\n",
    "print(\"Alignment model loaded\")\n",
    "\n",
    "# Load diarization model (you'll need to set your HuggingFace token)\n",
    "# Get your token from: https://huggingface.co/settings/tokens\n",
    "HF_TOKEN = os.getenv(\"HUGGINGFACE_TOKEN\") or \"YOUR_HF_TOKEN_HERE\"\n",
    "try:\n",
    "    diarize_model = whisperx.DiarizationPipeline(use_auth_token=HF_TOKEN, device=device)\n",
    "    print(\"Diarization model loaded\")\n",
    "except AttributeError:\n",
    "    # For newer versions of whisperx, use this approach\n",
    "    from pyannote.audio import Pipeline\n",
    "    diarize_model = Pipeline.from_pretrained(\n",
    "        \"pyannote/speaker-diarization-3.1\",\n",
    "        use_auth_token=HF_TOKEN\n",
    "    )\n",
    "    if device == \"cuda\":\n",
    "        diarize_model = diarize_model.to(torch.device(\"cuda\"))\n",
    "    print(\"Diarization model loaded (pyannote)\")\n",
    "\n",
    "print(\"All WhisperX models ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a21452c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio processing function ready!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# AUDIO PROCESSING FUNCTION\n",
    "# ============================================================\n",
    "\n",
    "def process_audio_file(audio_path, file_label):\n",
    "    \"\"\"\n",
    "    Complete audio processing pipeline:\n",
    "    1. Load and transcribe audio\n",
    "    2. Perform speaker diarization\n",
    "    3. Assign speakers to segments\n",
    "    4. Return processed results\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"PROCESSING: {file_label}\")\n",
    "    print(f\"File: {audio_path}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    try:\n",
    "        # Step 1: Load audio\n",
    "        print(\"Loading audio...\")\n",
    "        audio = whisperx.load_audio(str(audio_path))\n",
    "        \n",
    "        # Step 2: Transcribe\n",
    "        print(\"Transcribing...\")\n",
    "        result = model.transcribe(audio, batch_size=batch_size)\n",
    "        \n",
    "        # Step 3: Align whisper output\n",
    "        print(\"Aligning transcript...\")\n",
    "        result = whisperx.align(result[\"segments\"], model_a, metadata, audio, device, return_char_alignments=False)\n",
    "        \n",
    "        # Step 4: Diarize speakers\n",
    "        print(\"Performing speaker diarization...\")\n",
    "        try:\n",
    "            # Try whisperx approach first\n",
    "            diarize_segments = diarize_model(audio)\n",
    "        except:\n",
    "            # Use pyannote approach\n",
    "            import torchaudio\n",
    "            waveform, sample_rate = torchaudio.load(str(audio_path))\n",
    "            diarize_segments = diarize_model({\"waveform\": waveform, \"sample_rate\": sample_rate})\n",
    "        \n",
    "        # Step 5: Assign word speakers\n",
    "        print(\"Assigning speakers to segments...\")\n",
    "        result = whisperx.assign_word_speakers(diarize_segments, result)\n",
    "        \n",
    "        # Step 6: Create summary stats\n",
    "        speakers = sorted(list(set(seg.get('speaker', 'UNKNOWN') for seg in result['segments'] if seg.get('speaker'))))\n",
    "        total_duration = max(seg.get('end', 0) for seg in result['segments'])\n",
    "        \n",
    "        print(f\"Processing complete!\")\n",
    "        print(f\"  - Duration: {total_duration/60:.1f} minutes\")\n",
    "        print(f\"  - Speakers detected: {len(speakers)}\")\n",
    "        print(f\"  - Segments: {len(result['segments'])}\")\n",
    "        \n",
    "        return {\n",
    "            'label': file_label,\n",
    "            'result': result,\n",
    "            'speakers': speakers,\n",
    "            'duration': total_duration,\n",
    "            'audio_path': audio_path\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_label}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "print(\"Audio processing function ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d3902ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI client initialized\n",
      "Speaker identification functions ready!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# AI-POWERED SPEAKER IDENTIFICATION\n",
    "# ============================================================\n",
    "\n",
    "# Setup OpenAI client\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not api_key:\n",
    "    print(\"OPENAI_API_KEY not found. Speaker identification will be limited.\")\n",
    "    client = None\n",
    "else:\n",
    "    client = OpenAI(api_key=api_key)\n",
    "    print(\"OpenAI client initialized\")\n",
    "\n",
    "def suggest_speaker_identity(speaker_text, global_context=\"auto\", audio_label=\"\"):\n",
    "    \"\"\"\n",
    "    Advanced speaker identification using complete transcript analysis.\n",
    "    Analyzes full speaking patterns, vocabulary, and context clues.\n",
    "    \"\"\"\n",
    "    if not client:\n",
    "        return \"Unknown Speaker\"\n",
    "    \n",
    "    # Auto-detect context\n",
    "    if global_context == \"auto\":\n",
    "        global_context = detect_context(speaker_text)\n",
    "    \n",
    "    # Prepare analysis prompt\n",
    "    prompt = f\"\"\"\n",
    "You are an expert in political speech analysis and speaker identification.\n",
    "\n",
    "CONTEXT: {global_context}\n",
    "AUDIO SOURCE: {audio_label}\n",
    "\n",
    "TASK: Analyze this speaker's complete transcript and identify who they most likely are.\n",
    "\n",
    "FULL SPEAKER TRANSCRIPT:\n",
    "{speaker_text[:2000]}\n",
    "\n",
    "ANALYSIS INSTRUCTIONS:\n",
    "1. Look for distinctive speech patterns, phrases, and policy positions\n",
    "2. Identify characteristic vocabulary and speaking style\n",
    "3. Note any specific names, titles, or references mentioned\n",
    "4. Consider the political context and likely participants\n",
    "5. If this appears to be a well-known political figure, provide their name\n",
    "6. If uncertain, provide a descriptive role (Moderator, Candidate A, Journalist, etc.)\n",
    "\n",
    "Respond with ONLY the most likely identity or role:\n",
    "\"\"\"\n",
    "\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            max_tokens=50,\n",
    "            temperature=0.1,\n",
    "        )\n",
    "        return response.choices[0].message.content.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error in speaker identification: {e}\")\n",
    "        return \"Unknown Speaker\"\n",
    "\n",
    "def analyze_speakers_in_audio(processed_audio):\n",
    "    \"\"\"\n",
    "    Analyze and identify all speakers in a processed audio file.\n",
    "    \"\"\"\n",
    "    if not processed_audio:\n",
    "        return None\n",
    "    \n",
    "    result = processed_audio['result']\n",
    "    speakers = processed_audio['speakers']\n",
    "    label = processed_audio['label']\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"SPEAKER IDENTIFICATION: {label}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    speaker_analysis = {}\n",
    "    \n",
    "    for speaker in speakers:\n",
    "        print(f\"\\nAnalyzing {speaker}...\")\n",
    "        \n",
    "        # Gather all text for this speaker\n",
    "        speaker_segments = [seg for seg in result['segments'] if seg.get('speaker') == speaker]\n",
    "        speaker_text = \" \".join([seg.get('text', '').strip() for seg in speaker_segments])\n",
    "        \n",
    "        # Get speaker statistics\n",
    "        word_count = len(speaker_text.split())\n",
    "        segment_count = len(speaker_segments)\n",
    "        \n",
    "        # Get AI identification\n",
    "        identified_name = suggest_speaker_identity(\n",
    "            speaker_text, \n",
    "            global_context=\"auto\",\n",
    "            audio_label=label\n",
    "        )\n",
    "        \n",
    "        speaker_analysis[speaker] = {\n",
    "            'identified_as': identified_name,\n",
    "            'text': speaker_text,\n",
    "            'word_count': word_count,\n",
    "            'segment_count': segment_count,\n",
    "            'clean_text': clean_text(speaker_text)\n",
    "        }\n",
    "        \n",
    "        print(f\"  {speaker} â†’ {identified_name}\")\n",
    "        print(f\"  Word count: {word_count:,}\")\n",
    "        print(f\"  Segments: {segment_count}\")\n",
    "    \n",
    "    return speaker_analysis\n",
    "\n",
    "print(\"Speaker identification functions ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75dff6ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting multi-audio processing...\n",
      "Found 3 audio files to process\n",
      "\n",
      "============================================================\n",
      "PROCESSING: US_Debate\n",
      "File: ..\\data\\US_DebateAudio.wav\n",
      "============================================================\n",
      "Loading audio...\n",
      "Transcribing...\n",
      "Transcribing...\n",
      "2025-11-24 20:02:57 - whisperx.asr - INFO - Detected language: en (0.99) in first 30s of audio\n",
      "2025-11-24 20:02:57 - whisperx.asr - INFO - Detected language: en (0.99) in first 30s of audio\n",
      "Aligning transcript...\n",
      "Aligning transcript...\n",
      "Performing speaker diarization...\n",
      "Performing speaker diarization...\n",
      "Assigning speakers to segments...\n",
      "Error processing US_Debate: 'e'\n",
      "\n",
      "============================================================\n",
      "PROCESSING: Irish_Presidential\n",
      "File: ..\\data\\IrishPresidentalElection.wav\n",
      "============================================================\n",
      "Loading audio...\n",
      "Assigning speakers to segments...\n",
      "Error processing US_Debate: 'e'\n",
      "\n",
      "============================================================\n",
      "PROCESSING: Irish_Presidential\n",
      "File: ..\\data\\IrishPresidentalElection.wav\n",
      "============================================================\n",
      "Loading audio...\n",
      "Transcribing...\n",
      "Transcribing...\n",
      "2025-11-24 20:32:31 - whisperx.asr - INFO - Detected language: en (0.99) in first 30s of audio\n",
      "2025-11-24 20:32:31 - whisperx.asr - INFO - Detected language: en (0.99) in first 30s of audio\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# PROCESS ALL AUDIO FILES\n",
    "# ============================================================\n",
    "\n",
    "# Store results for all audio files\n",
    "all_results = {}\n",
    "all_speaker_analyses = {}\n",
    "\n",
    "print(\"Starting multi-audio processing...\")\n",
    "print(f\"Found {len(AUDIO_FILES)} audio files to process\")\n",
    "\n",
    "for audio_name, audio_path in AUDIO_FILES.items():\n",
    "    if audio_path.exists():\n",
    "        # Process audio file\n",
    "        processed = process_audio_file(audio_path, audio_name)\n",
    "        \n",
    "        if processed:\n",
    "            all_results[audio_name] = processed\n",
    "            \n",
    "            # Analyze speakers in this audio\n",
    "            speaker_analysis = analyze_speakers_in_audio(processed)\n",
    "            if speaker_analysis:\n",
    "                all_speaker_analyses[audio_name] = speaker_analysis\n",
    "        \n",
    "        # Clean up GPU memory\n",
    "        if device == \"cuda\":\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "    else:\n",
    "        print(f\"Audio file not found: {audio_path}\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"MULTI-AUDIO PROCESSING COMPLETE\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Successfully processed: {len(all_results)} audio files\")\n",
    "for name in all_results.keys():\n",
    "    print(f\" {name}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
