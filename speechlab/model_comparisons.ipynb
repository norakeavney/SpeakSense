{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01ed2e38",
   "metadata": {},
   "source": [
    "# Model Comparison: WhisperX vs OpenAI Whisper vs Pyannote\n",
    "\n",
    "## Project Overview\n",
    "This notebook benchmarks different speech recognition and diarization models to determine:\n",
    "- **Which is most accurate?**\n",
    "- **Which is fastest?**\n",
    "- **Which is best for our bias detection use case?**\n",
    "\n",
    "## Models We're Testing\n",
    "\n",
    "### Transcription Models:\n",
    "1. **WhisperX** - Our current choice\n",
    "   - Pros: Word-level timestamps, alignment, fast\n",
    "   - Cons: Additional processing steps\n",
    "   \n",
    "2. **OpenAI Whisper (base)** - The original\n",
    "   - Pros: Simple, well-documented, accurate\n",
    "   - Cons: Slower, phrase-level timestamps only\n",
    "\n",
    "### Diarization:\n",
    "3. **pyannote.audio 3.1** - Current speaker identification method\n",
    "   - Pros: Industry standard, very accurate\n",
    "   - Cons: Requires Hugging Face token, slower\n",
    "\n",
    "## Evaluation Metrics\n",
    "- **Processing time** - How long does it take?\n",
    "- **Transcription accuracy** - Word Error Rate (WER)\n",
    "- **Diarization accuracy** - Speaker identification quality\n",
    "- **Memory usage** - Resource consumption\n",
    "- **Ease of use** - Setup complexity\n",
    "\n",
    "## Test Audio\n",
    "We'll use our 9-minute US Debate audio file for a fair comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "506235ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai-whisper in c:\\users\\norak\\speaksense\\venv\\lib\\site-packages (20250625)\n",
      "Requirement already satisfied: whisperx in c:\\users\\norak\\speaksense\\venv\\lib\\site-packages (3.7.4)\n",
      "Collecting jiwer\n",
      "  Downloading jiwer-4.0.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: pandas in c:\\users\\norak\\speaksense\\venv\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\norak\\speaksense\\venv\\lib\\site-packages (3.10.7)\n",
      "Requirement already satisfied: more-itertools in c:\\users\\norak\\speaksense\\venv\\lib\\site-packages (from openai-whisper) (10.8.0)\n",
      "Requirement already satisfied: numba in c:\\users\\norak\\speaksense\\venv\\lib\\site-packages (from openai-whisper) (0.62.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\norak\\speaksense\\venv\\lib\\site-packages (from openai-whisper) (2.0.2)\n",
      "Requirement already satisfied: tiktoken in c:\\users\\norak\\speaksense\\venv\\lib\\site-packages (from openai-whisper) (0.12.0)\n",
      "Requirement already satisfied: torch in c:\\users\\norak\\speaksense\\venv\\lib\\site-packages (from openai-whisper) (2.8.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\norak\\speaksense\\venv\\lib\\site-packages (from openai-whisper) (4.67.1)\n",
      "Requirement already satisfied: ctranslate2>=4.5.0 in c:\\users\\norak\\speaksense\\venv\\lib\\site-packages (from whisperx) (4.6.0)\n",
      "Requirement already satisfied: faster-whisper>=1.1.1 in c:\\users\\norak\\speaksense\\venv\\lib\\site-packages (from whisperx) (1.2.1)\n",
      "Requirement already satisfied: nltk>=3.9.1 in c:\\users\\norak\\speaksense\\venv\\lib\\site-packages (from whisperx) (3.9.2)\n",
      "Requirement already satisfied: av<16.0.0 in c:\\users\\norak\\speaksense\\venv\\lib\\site-packages (from whisperx) (15.1.0)\n",
      "Requirement already satisfied: pyannote-audio<4.0.0,>=3.3.2 in c:\\users\\norak\\speaksense\\venv\\lib\\site-packages (from whisperx) (3.4.0)\n",
      "Requirement already satisfied: torchaudio~=2.8.0 in c:\\users\\norak\\speaksense\\venv\\lib\\site-packages (from whisperx) (2.8.0)\n",
      "Requirement already satisfied: transformers>=4.48.0 in c:\\users\\norak\\speaksense\\venv\\lib\\site-packages (from whisperx) (4.57.1)\n",
      "Requirement already satisfied: click>=8.1.8 in c:\\users\\norak\\speaksense\\venv\\lib\\site-packages (from jiwer) (8.3.0)\n",
      "Collecting rapidfuzz>=3.9.7 (from jiwer)\n",
      "  Downloading rapidfuzz-3.14.3-cp312-cp312-win_amd64.whl.metadata (12 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\norak\\speaksense\\venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\norak\\speaksense\\venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\norak\\speaksense\\venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\norak\\speaksense\\venv\\lib\\site-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\norak\\speaksense\\venv\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\norak\\speaksense\\venv\\lib\\site-packages (from matplotlib) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\norak\\speaksense\\venv\\lib\\site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\norak\\speaksense\\venv\\lib\\site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\norak\\speaksense\\venv\\lib\\site-packages (from matplotlib) (12.0.0)\n",
      "Requirement already satisfied: pyparsing>=3 in c:\\users\\norak\\speaksense\\venv\\lib\\site-packages (from matplotlib) (3.2.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\norak\\speaksense\\venv\\lib\\site-packages (from click>=8.1.8->jiwer) (0.4.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\norak\\speaksense\\venv\\lib\\site-packages (from ctranslate2>=4.5.0->whisperx) (80.9.0)\n",
      "Requirement already satisfied: pyyaml<7,>=5.3 in c:\\users\\norak\\speaksense\\venv\\lib\\site-packages (from ctranslate2>=4.5.0->whisperx) (6.0.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.21 in c:\\users\\norak\\speaksense\\venv\\lib\\site-packages (from faster-whisper>=1.1.1->whisperx) (0.36.0)\n",
      "Requirement already satisfied: tokenizers<1,>=0.13 in c:\\users\\norak\\speaksense\\venv\\lib\\site-packages (from faster-whisper>=1.1.1->whisperx) (0.22.1)\n",
      "Requirement already satisfied: onnxruntime<2,>=1.14 in c:\\users\\norak\\speaksense\\venv\\lib\\site-packages (from faster-whisper>=1.1.1->whisperx) (1.23.2)\n",
      "Requirement already satisfied: joblib in c:\\users\\norak\\speaksense\\venv\\lib\\site-packages (from nltk>=3.9.1->whisperx) (1.5.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\norak\\speaksense\\venv\\lib\\site-packages (from nltk>=3.9.1->whisperx) (2025.10.23)\n",
      "Requirement already satisfied: asteroid-filterbanks>=0.4 in c:\\users\\norak\\speaksense\\venv\\lib\\site-packages (from pyannote-audio<4.0.0,>=3.3.2->whisperx) (0.4.0)\n",
      "Requirement already satisfied: einops>=0.6.0 in c:\\users\\norak\\speaksense\\venv\\lib\\site-packages (from pyannote-audio<4.0.0,>=3.3.2->whisperx) (0.8.1)\n",
      "Requirement already satisfied: lightning>=2.0.1 in c:\\users\\norak\\speaksense\\venv\\lib\\site-packages (from pyannote-audio<4.0.0,>=3.3.2->whisperx) (2.5.5)\n",
      "Requirement already satisfied: omegaconf<3.0,>=2.1 in c:\\users\\norak\\speaksense\\venv\\lib\\site-packages (from pyannote-audio<4.0.0,>=3.3.2->whisperx) (2.3.0)\n",
      "Requirement already satisfied: pyannote.core<6.0,>=5.0.0 in c:\\users\\norak\\speaksense\\venv\\lib\\site-packages (from pyannote-audio<4.0.0,>=3.3.2->whisperx) (5.0.0)\n",
      "Requirement already satisfied: pyannote.database<6.0,>=5.0.1 in c:\\users\\norak\\speaksense\\venv\\lib\\site-packages (from pyannote-audio<4.0.0,>=3.3.2->whisperx) (5.1.3)\n",
      "Requirement already satisfied: pyannote.metrics<4.0,>=3.2 in c:\\users\\norak\\speaksense\\venv\\lib\\site-packages (from pyannote-audio<4.0.0,>=3.3.2->whisperx) (3.2.1)\n",
      "Requirement already satisfied: pyannote.pipeline<4.0,>=3.0.1 in c:\\users\\norak\\speaksense\\venv\\lib\\site-packages (from pyannote-audio<4.0.0,>=3.3.2->whisperx) (3.0.1)\n",
      "Requirement already satisfied: pytorch_metric_learning>=2.1.0 in c:\\users\\norak\\speaksense\\venv\\lib\\site-packages (from pyannote-audio<4.0.0,>=3.3.2->whisperx) (2.9.0)\n",
      "Requirement already satisfied: rich>=12.0.0 in c:\\users\\norak\\speaksense\\venv\\lib\\site-packages (from pyannote-audio<4.0.0,>=3.3.2->whisperx) (14.2.0)\n",
      "Requirement already satisfied: semver>=3.0.0 in c:\\users\\norak\\speaksense\\venv\\lib\\site-packages (from pyannote-audio<4.0.0,>=3.3.2->whisperx) (3.0.4)\n",
      "Requirement already satisfied: soundfile>=0.12.1 in c:\\users\\norak\\speaksense\\venv\\lib\\site-packages (from pyannote-audio<4.0.0,>=3.3.2->whisperx) (0.13.1)\n",
      "Requirement already satisfied: speechbrain>=1.0.0 in c:\\users\\norak\\speaksense\\venv\\lib\\site-packages (from pyannote-audio<4.0.0,>=3.3.2->whisperx) (1.0.3)\n",
      "Requirement already satisfied: tensorboardX>=2.6 in c:\\users\\norak\\speaksense\\venv\\lib\\site-packages (from pyannote-audio<4.0.0,>=3.3.2->whisperx) (2.6.4)\n",
      "Requirement already satisfied: torch_audiomentations>=0.11.0 in c:\\users\\norak\\speaksense\\venv\\lib\\site-packages (from pyannote-audio<4.0.0,>=3.3.2->whisperx) (0.12.0)\n",
      "Requirement already satisfied: torchmetrics>=0.11.0 in c:\\users\\norak\\speaksense\\venv\\lib\\site-packages (from pyannote-audio<4.0.0,>=3.3.2->whisperx) (1.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\norak\\speaksense\\venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\norak\\speaksense\\venv\\lib\\site-packages (from torch->openai-whisper) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\norak\\speaksense\\venv\\lib\\site-packages (from torch->openai-whisper) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\norak\\speaksense\\venv\\lib\\site-packages (from torch->openai-whisper) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\norak\\speaksense\\venv\\lib\\site-packages (from torch->openai-whisper) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\norak\\speaksense\\venv\\lib\\site-packages (from torch->openai-whisper) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\norak\\speaksense\\venv\\lib\\site-packages (from torch->openai-whisper) (2025.10.0)\n",
      "Requirement already satisfied: requests in c:\\users\\norak\\speaksense\\venv\\lib\\site-packages (from transformers>=4.48.0->whisperx) (2.32.5)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\norak\\speaksense\\venv\\lib\\site-packages (from transformers>=4.48.0->whisperx) (0.6.2)\n",
      "Requirement already satisfied: llvmlite<0.46,>=0.45.0dev0 in c:\\users\\norak\\speaksense\\venv\\lib\\site-packages (from numba->openai-whisper) (0.45.1)\n",
      "Requirement already satisfied: lightning-utilities<2.0,>=0.10.0 in c:\\users\\norak\\speaksense\\venv\\lib\\site-packages (from lightning>=2.0.1->pyannote-audio<4.0.0,>=3.3.2->whisperx) (0.15.2)\n",
      "Requirement already satisfied: pytorch-lightning in c:\\users\\norak\\speaksense\\venv\\lib\\site-packages (from lightning>=2.0.1->pyannote-audio<4.0.0,>=3.3.2->whisperx) (2.5.5)\n",
      "Requirement already satisfied: antlr4-python3-runtime==4.9.* in c:\\users\\norak\\speaksense\\venv\\lib\\site-packages (from omegaconf<3.0,>=2.1->pyannote-audio<4.0.0,>=3.3.2->whisperx) (4.9.3)\n",
      "Requirement already satisfied: coloredlogs in c:\\users\\norak\\speaksense\\venv\\lib\\site-packages (from onnxruntime<2,>=1.14->faster-whisper>=1.1.1->whisperx) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in c:\\users\\norak\\speaksense\\venv\\lib\\site-packages (from onnxruntime<2,>=1.14->faster-whisper>=1.1.1->whisperx) (25.9.23)\n",
      "Requirement already satisfied: protobuf in c:\\users\\norak\\speaksense\\venv\\lib\\site-packages (from onnxruntime<2,>=1.14->faster-whisper>=1.1.1->whisperx) (6.33.0)\n",
      "Requirement already satisfied: sortedcontainers>=2.0.4 in c:\\users\\norak\\speaksense\\venv\\lib\\site-packages (from pyannote.core<6.0,>=5.0.0->pyannote-audio<4.0.0,>=3.3.2->whisperx) (2.4.0)\n",
      "Requirement already satisfied: scipy>=1.1 in c:\\users\\norak\\speaksense\\venv\\lib\\site-packages (from pyannote.core<6.0,>=5.0.0->pyannote-audio<4.0.0,>=3.3.2->whisperx) (1.16.3)\n",
      "Requirement already satisfied: typer>=0.12.1 in c:\\users\\norak\\speaksense\\venv\\lib\\site-packages (from pyannote.database<6.0,>=5.0.1->pyannote-audio<4.0.0,>=3.3.2->whisperx) (0.20.0)\n",
      "Requirement already satisfied: scikit-learn>=0.17.1 in c:\\users\\norak\\speaksense\\venv\\lib\\site-packages (from pyannote.metrics<4.0,>=3.2->pyannote-audio<4.0.0,>=3.3.2->whisperx) (1.7.2)\n",
      "Requirement already satisfied: docopt>=0.6.2 in c:\\users\\norak\\speaksense\\venv\\lib\\site-packages (from pyannote.metrics<4.0,>=3.2->pyannote-audio<4.0.0,>=3.3.2->whisperx) (0.6.2)\n",
      "Requirement already satisfied: tabulate>=0.7.7 in c:\\users\\norak\\speaksense\\venv\\lib\\site-packages (from pyannote.metrics<4.0,>=3.2->pyannote-audio<4.0.0,>=3.3.2->whisperx) (0.9.0)\n",
      "Requirement already satisfied: optuna>=3.1 in c:\\users\\norak\\speaksense\\venv\\lib\\site-packages (from pyannote.pipeline<4.0,>=3.0.1->pyannote-audio<4.0.0,>=3.3.2->whisperx) (4.5.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\norak\\speaksense\\venv\\lib\\site-packages (from requests->transformers>=4.48.0->whisperx) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\norak\\speaksense\\venv\\lib\\site-packages (from requests->transformers>=4.48.0->whisperx) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\norak\\speaksense\\venv\\lib\\site-packages (from requests->transformers>=4.48.0->whisperx) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\norak\\speaksense\\venv\\lib\\site-packages (from requests->transformers>=4.48.0->whisperx) (2025.10.5)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\norak\\speaksense\\venv\\lib\\site-packages (from rich>=12.0.0->pyannote-audio<4.0.0,>=3.3.2->whisperx) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\norak\\speaksense\\venv\\lib\\site-packages (from rich>=12.0.0->pyannote-audio<4.0.0,>=3.3.2->whisperx) (2.19.2)\n",
      "Requirement already satisfied: cffi>=1.0 in c:\\users\\norak\\speaksense\\venv\\lib\\site-packages (from soundfile>=0.12.1->pyannote-audio<4.0.0,>=3.3.2->whisperx) (2.0.0)\n",
      "Requirement already satisfied: hyperpyyaml in c:\\users\\norak\\speaksense\\venv\\lib\\site-packages (from speechbrain>=1.0.0->pyannote-audio<4.0.0,>=3.3.2->whisperx) (1.2.2)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\norak\\speaksense\\venv\\lib\\site-packages (from speechbrain>=1.0.0->pyannote-audio<4.0.0,>=3.3.2->whisperx) (0.2.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\norak\\speaksense\\venv\\lib\\site-packages (from sympy>=1.13.3->torch->openai-whisper) (1.3.0)\n",
      "Requirement already satisfied: julius<0.3,>=0.2.3 in c:\\users\\norak\\speaksense\\venv\\lib\\site-packages (from torch_audiomentations>=0.11.0->pyannote-audio<4.0.0,>=3.3.2->whisperx) (0.2.7)\n",
      "Requirement already satisfied: torch-pitch-shift>=1.2.2 in c:\\users\\norak\\speaksense\\venv\\lib\\site-packages (from torch_audiomentations>=0.11.0->pyannote-audio<4.0.0,>=3.3.2->whisperx) (1.2.5)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\norak\\speaksense\\venv\\lib\\site-packages (from jinja2->torch->openai-whisper) (3.0.3)\n",
      "Requirement already satisfied: pycparser in c:\\users\\norak\\speaksense\\venv\\lib\\site-packages (from cffi>=1.0->soundfile>=0.12.1->pyannote-audio<4.0.0,>=3.3.2->whisperx) (2.23)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\norak\\speaksense\\venv\\lib\\site-packages (from fsspec[http]<2027.0,>=2022.5.0->lightning>=2.0.1->pyannote-audio<4.0.0,>=3.3.2->whisperx) (3.13.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\norak\\speaksense\\venv\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=12.0.0->pyannote-audio<4.0.0,>=3.3.2->whisperx) (0.1.2)\n",
      "Requirement already satisfied: alembic>=1.5.0 in c:\\users\\norak\\speaksense\\venv\\lib\\site-packages (from optuna>=3.1->pyannote.pipeline<4.0,>=3.0.1->pyannote-audio<4.0.0,>=3.3.2->whisperx) (1.17.1)\n",
      "Requirement already satisfied: colorlog in c:\\users\\norak\\speaksense\\venv\\lib\\site-packages (from optuna>=3.1->pyannote.pipeline<4.0,>=3.0.1->pyannote-audio<4.0.0,>=3.3.2->whisperx) (6.10.1)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.2 in c:\\users\\norak\\speaksense\\venv\\lib\\site-packages (from optuna>=3.1->pyannote.pipeline<4.0,>=3.0.1->pyannote-audio<4.0.0,>=3.3.2->whisperx) (2.0.44)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\norak\\speaksense\\venv\\lib\\site-packages (from scikit-learn>=0.17.1->pyannote.metrics<4.0,>=3.2->pyannote-audio<4.0.0,>=3.3.2->whisperx) (3.6.0)\n",
      "Requirement already satisfied: primePy>=1.3 in c:\\users\\norak\\speaksense\\venv\\lib\\site-packages (from torch-pitch-shift>=1.2.2->torch_audiomentations>=0.11.0->pyannote-audio<4.0.0,>=3.3.2->whisperx) (1.3)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\norak\\speaksense\\venv\\lib\\site-packages (from typer>=0.12.1->pyannote.database<6.0,>=5.0.1->pyannote-audio<4.0.0,>=3.3.2->whisperx) (1.5.4)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in c:\\users\\norak\\speaksense\\venv\\lib\\site-packages (from coloredlogs->onnxruntime<2,>=1.14->faster-whisper>=1.1.1->whisperx) (10.0)\n",
      "Requirement already satisfied: ruamel.yaml>=0.17.28 in c:\\users\\norak\\speaksense\\venv\\lib\\site-packages (from hyperpyyaml->speechbrain>=1.0.0->pyannote-audio<4.0.0,>=3.3.2->whisperx) (0.18.16)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\norak\\speaksense\\venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning>=2.0.1->pyannote-audio<4.0.0,>=3.3.2->whisperx) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in c:\\users\\norak\\speaksense\\venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning>=2.0.1->pyannote-audio<4.0.0,>=3.3.2->whisperx) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\norak\\speaksense\\venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning>=2.0.1->pyannote-audio<4.0.0,>=3.3.2->whisperx) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\norak\\speaksense\\venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning>=2.0.1->pyannote-audio<4.0.0,>=3.3.2->whisperx) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\norak\\speaksense\\venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning>=2.0.1->pyannote-audio<4.0.0,>=3.3.2->whisperx) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\norak\\speaksense\\venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning>=2.0.1->pyannote-audio<4.0.0,>=3.3.2->whisperx) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\norak\\speaksense\\venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning>=2.0.1->pyannote-audio<4.0.0,>=3.3.2->whisperx) (1.22.0)\n",
      "Requirement already satisfied: Mako in c:\\users\\norak\\speaksense\\venv\\lib\\site-packages (from alembic>=1.5.0->optuna>=3.1->pyannote.pipeline<4.0,>=3.0.1->pyannote-audio<4.0.0,>=3.3.2->whisperx) (1.3.10)\n",
      "Requirement already satisfied: pyreadline3 in c:\\users\\norak\\speaksense\\venv\\lib\\site-packages (from humanfriendly>=9.1->coloredlogs->onnxruntime<2,>=1.14->faster-whisper>=1.1.1->whisperx) (3.5.4)\n",
      "Requirement already satisfied: ruamel.yaml.clib>=0.2.7 in c:\\users\\norak\\speaksense\\venv\\lib\\site-packages (from ruamel.yaml>=0.17.28->hyperpyyaml->speechbrain>=1.0.0->pyannote-audio<4.0.0,>=3.3.2->whisperx) (0.2.14)\n",
      "Requirement already satisfied: greenlet>=1 in c:\\users\\norak\\speaksense\\venv\\lib\\site-packages (from sqlalchemy>=1.4.2->optuna>=3.1->pyannote.pipeline<4.0,>=3.0.1->pyannote-audio<4.0.0,>=3.3.2->whisperx) (3.2.4)\n",
      "Downloading jiwer-4.0.0-py3-none-any.whl (23 kB)\n",
      "Downloading rapidfuzz-3.14.3-cp312-cp312-win_amd64.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.1/1.5 MB 1.5 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 0.5/1.5 MB 5.4 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 1.0/1.5 MB 7.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  1.5/1.5 MB 8.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.5/1.5 MB 7.5 MB/s eta 0:00:00\n",
      "Installing collected packages: rapidfuzz, jiwer\n",
      "Successfully installed jiwer-4.0.0 rapidfuzz-3.14.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# Install all required packages for comparison\n",
    "# Run this once\n",
    "\n",
    "import pip\n",
    "\n",
    "!pip install openai-whisper whisperx jiwer pandas matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d81c39c",
   "metadata": {},
   "source": [
    "## Step One: Import Libraries and Setup\n",
    "\n",
    "We'll import both Whisper models and prepare for benchmarking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "504d9127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported\n",
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import whisper  # Original OpenAI Whisper\n",
    "import whisperx  # WhisperX (our current choice)\n",
    "import torch\n",
    "import time\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from jiwer import wer  # Word Error Rate calculation\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
    "\n",
    "# Check device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Libraries imported\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Prepare results storage\n",
    "results = {\n",
    "    'model': [],\n",
    "    'transcription_time': [],\n",
    "    'alignment_time': [],\n",
    "    'diarization_time': [],\n",
    "    'total_time': [],\n",
    "    'segments_found': [],\n",
    "    'memory_usage': []\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2990f4c",
   "metadata": {},
   "source": [
    "## Step Two: Test OpenAI Whisper (Original)\n",
    "\n",
    "Let's benchmark the **original** Whisper model first.\n",
    "\n",
    "**What we're timing:**\n",
    "- Model loading time\n",
    "- Transcription time\n",
    "- Total processing time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "671f3cd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TEST 1: OpenAI Whisper\n",
      "============================================================\n",
      "\n",
      "Loading OpenAI Whisper model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 139M/139M [00:13<00:00, 10.5MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded in 15.45 seconds\n",
      "\n",
      "Transcribing audio...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\norak\\SpeakSense\\venv\\Lib\\site-packages\\whisper\\transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected language: English\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 56232/56577 [01:29<00:00, 627.00frames/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription complete in 92.78 seconds\n",
      "\n",
      "Results:\n",
      "  Total time: 92.78s\n",
      "  Segments found: 154\n",
      "  Text preview:  She doesn't have a plan. She copied Biden's plan and it's like four sentences, like run-spot-run, four sentences that are just, oh, we'll try in lower taxes. She doesn't have a plan. Take a look at h...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"TEST 1: OpenAI Whisper\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Path to test audio\n",
    "audio_file = \"../data/US_DebateAudio.wav\"\n",
    "\n",
    "# TIME: Model loading\n",
    "print(\"\\nLoading OpenAI Whisper model...\")\n",
    "start_load = time.time()\n",
    "whisper_model = whisper.load_model(\"base\", device=device)\n",
    "load_time = time.time() - start_load\n",
    "print(f\"Model loaded in {load_time:.2f} seconds\")\n",
    "\n",
    "# TIME: Transcription\n",
    "print(\"\\nTranscribing audio...\")\n",
    "start_transcribe = time.time()\n",
    "whisper_result = whisper_model.transcribe(audio_file, verbose=False)\n",
    "transcribe_time = time.time() - start_transcribe\n",
    "print(f\"Transcription complete in {transcribe_time:.2f} seconds\")\n",
    "\n",
    "# Store results\n",
    "results['model'].append('OpenAI Whisper')\n",
    "results['transcription_time'].append(transcribe_time)\n",
    "results['alignment_time'].append(0)  # No alignment step\n",
    "results['diarization_time'].append(0)  # No diarization\n",
    "results['total_time'].append(transcribe_time)\n",
    "results['segments_found'].append(len(whisper_result['segments']))\n",
    "results['memory_usage'].append('N/A')  # Can add psutil for this\n",
    "\n",
    "print(f\"\\nResults:\")\n",
    "print(f\"  Total time: {transcribe_time:.2f}s\")\n",
    "print(f\"  Segments found: {len(whisper_result['segments'])}\")\n",
    "print(f\"  Text preview: {whisper_result['text'][:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cdc0248",
   "metadata": {},
   "source": [
    "## Step 3: Test WhisperX (Current Method)\n",
    "\n",
    "Now let's benchmark our current pipeline with alignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4479cbeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TEST 2: WhisperX with Alignment\n",
      "============================================================\n",
      "\n",
      "[1/3] Loading WhisperX model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\norak\\SpeakSense\\venv\\Lib\\site-packages\\ctranslate2\\__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\n",
      "c:\\Users\\norak\\SpeakSense\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\norak\\SpeakSense\\venv\\Lib\\site-packages\\pyannote\\audio\\core\\io.py:212: UserWarning: torchaudio._backend.list_audio_backends has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n",
      "  torchaudio.list_audio_backends()\n",
      "c:\\Users\\norak\\SpeakSense\\venv\\Lib\\site-packages\\speechbrain\\utils\\torch_audio_backend.py:57: UserWarning: torchaudio._backend.list_audio_backends has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n",
      "  available_backends = torchaudio.list_audio_backends()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-11 11:40:23 - whisperx.asr - INFO - No language specified, language will be detected for each audio file (increases inference time)\n",
      "2025-11-11 11:40:23 - whisperx.vads.pyannote - INFO - Performing voice activity detection using Pyannote...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python312\\Lib\\inspect.py:1001: UserWarning: Module 'speechbrain.pretrained' was deprecated, redirecting to 'speechbrain.inference'. Please update your script. This is a change from SpeechBrain 1.0. See: https://github.com/speechbrain/speechbrain/releases/tag/v1.0.0\n",
      "  if ismodule(module) and hasattr(module, '__file__'):\n",
      "Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.5.5. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint c:\\Users\\norak\\SpeakSense\\venv\\Lib\\site-packages\\whisperx\\assets\\pytorch_model.bin`\n",
      "c:\\Users\\norak\\SpeakSense\\venv\\Lib\\site-packages\\pyannote\\audio\\core\\io.py:212: UserWarning: torchaudio._backend.list_audio_backends has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n",
      "  torchaudio.list_audio_backends()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model was trained with pyannote.audio 0.0.1, yours is 3.4.0. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
      "Model was trained with torch 1.10.0+cu102, yours is 2.8.0+cpu. Bad things might happen unless you revert torch to 1.x.\n",
      "✓ Model loaded in 68.52 seconds\n",
      "\n",
      "[2/3] Transcribing audio...\n",
      "2025-11-11 11:40:48 - whisperx.asr - INFO - Detected language: en (0.99) in first 30s of audio\n",
      "✓ Transcription complete in 72.40 seconds\n",
      "\n",
      "[3/3] Aligning timestamps to word level...\n",
      "✓ Alignment complete in 118.09 seconds\n",
      "\n",
      "Results:\n",
      "  Transcription time: 72.40s\n",
      "  Alignment time: 118.09s\n",
      "  Total time: 190.49s\n",
      "  Segments found: 112\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"TEST 2: WhisperX with Alignment\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# TIME: Model loading\n",
    "print(\"\\n[1/3] Loading WhisperX model...\")\n",
    "start_load = time.time()\n",
    "whisperx_model = whisperx.load_model(\"base\", device=device, compute_type=\"int8\" if device==\"cpu\" else \"float16\")\n",
    "load_time = time.time() - start_load\n",
    "print(f\"✓ Model loaded in {load_time:.2f} seconds\")\n",
    "\n",
    "# TIME: Transcription\n",
    "print(\"\\n[2/3] Transcribing audio...\")\n",
    "start_transcribe = time.time()\n",
    "whisperx_result = whisperx_model.transcribe(audio_file)\n",
    "transcribe_time = time.time() - start_transcribe\n",
    "print(f\"✓ Transcription complete in {transcribe_time:.2f} seconds\")\n",
    "\n",
    "# TIME: Alignment\n",
    "print(\"\\n[3/3] Aligning timestamps to word level...\")\n",
    "start_align = time.time()\n",
    "model_a, metadata = whisperx.load_align_model(language_code=\"en\", device=device)\n",
    "whisperx_result = whisperx.align(whisperx_result[\"segments\"], model_a, metadata, audio_file, device=device)\n",
    "align_time = time.time() - start_align\n",
    "print(f\"✓ Alignment complete in {align_time:.2f} seconds\")\n",
    "\n",
    "# Store results\n",
    "total_time = transcribe_time + align_time\n",
    "results['model'].append('WhisperX + Alignment')\n",
    "results['transcription_time'].append(transcribe_time)\n",
    "results['alignment_time'].append(align_time)\n",
    "results['diarization_time'].append(0)  # No diarization yet\n",
    "results['total_time'].append(total_time)\n",
    "results['segments_found'].append(len(whisperx_result['segments']))\n",
    "results['memory_usage'].append('N/A')\n",
    "\n",
    "print(f\"\\nResults:\")\n",
    "print(f\"  Transcription time: {transcribe_time:.2f}s\")\n",
    "print(f\"  Alignment time: {align_time:.2f}s\")\n",
    "print(f\"  Total time: {total_time:.2f}s\")\n",
    "print(f\"  Segments found: {len(whisperx_result['segments'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082b2f93",
   "metadata": {},
   "source": [
    "## Step Four: Test Full Pipeline (WhisperX + Diarization)\n",
    "\n",
    "Finally, let's test the complete pipeline including speaker identification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fec29c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TEST 3: WhisperX + Alignment + Diarization\n",
      "============================================================\n",
      "\n",
      "[1/2] Loading diarization model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\norak\\SpeakSense\\venv\\Lib\\site-packages\\pyannote\\audio\\core\\io.py:212: UserWarning: torchaudio._backend.list_audio_backends has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n",
      "  torchaudio.list_audio_backends()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded in 2.31 seconds\n",
      "\n",
      "[2/2] Running speaker diarization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\norak\\SpeakSense\\venv\\Lib\\site-packages\\torchaudio\\_backend\\utils.py:213: UserWarning: In 2.9, this function's implementation will be changed to use torchaudio.load_with_torchcodec` under the hood. Some parameters like ``normalize``, ``format``, ``buffer_size``, and ``backend`` will be ignored. We recommend that you port your code to rely directly on TorchCodec's decoder instead: https://docs.pytorch.org/torchcodec/stable/generated/torchcodec.decoders.AudioDecoder.html#torchcodec.decoders.AudioDecoder.\n",
      "  warnings.warn(\n",
      "c:\\Users\\norak\\SpeakSense\\venv\\Lib\\site-packages\\pyannote\\audio\\models\\blocks\\pooling.py:104: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\native\\ReduceOps.cpp:1839.)\n",
      "  std = sequences.std(dim=-1, correction=1)\n",
      "c:\\Users\\norak\\SpeakSense\\venv\\Lib\\site-packages\\pyannote\\audio\\core\\io.py:85: UserWarning: torchaudio._backend.utils.info has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n",
      "  info = torchaudio.info(file[\"audio\"], backend=backend)\n",
      "c:\\Users\\norak\\SpeakSense\\venv\\Lib\\site-packages\\torchaudio\\_backend\\soundfile_backend.py:120: UserWarning: torchaudio._backend.common.AudioMetaData has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n",
      "  return AudioMetaData(\n",
      "c:\\Users\\norak\\SpeakSense\\venv\\Lib\\site-packages\\pyannote\\audio\\core\\io.py:85: UserWarning: torchaudio._backend.utils.info has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n",
      "  info = torchaudio.info(file[\"audio\"], backend=backend)\n",
      "c:\\Users\\norak\\SpeakSense\\venv\\Lib\\site-packages\\torchaudio\\_backend\\soundfile_backend.py:120: UserWarning: torchaudio._backend.common.AudioMetaData has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n",
      "  return AudioMetaData(\n",
      "c:\\Users\\norak\\SpeakSense\\venv\\Lib\\site-packages\\torchaudio\\_backend\\utils.py:213: UserWarning: In 2.9, this function's implementation will be changed to use torchaudio.load_with_torchcodec` under the hood. Some parameters like ``normalize``, ``format``, ``buffer_size``, and ``backend`` will be ignored. We recommend that you port your code to rely directly on TorchCodec's decoder instead: https://docs.pytorch.org/torchcodec/stable/generated/torchcodec.decoders.AudioDecoder.html#torchcodec.decoders.AudioDecoder.\n",
      "  warnings.warn(\n",
      "c:\\Users\\norak\\SpeakSense\\venv\\Lib\\site-packages\\pyannote\\audio\\core\\io.py:85: UserWarning: torchaudio._backend.utils.info has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n",
      "  info = torchaudio.info(file[\"audio\"], backend=backend)\n",
      "c:\\Users\\norak\\SpeakSense\\venv\\Lib\\site-packages\\torchaudio\\_backend\\soundfile_backend.py:120: UserWarning: torchaudio._backend.common.AudioMetaData has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n",
      "  return AudioMetaData(\n",
      "c:\\Users\\norak\\SpeakSense\\venv\\Lib\\site-packages\\torchaudio\\_backend\\utils.py:213: UserWarning: In 2.9, this function's implementation will be changed to use torchaudio.load_with_torchcodec` under the hood. Some parameters like ``normalize``, ``format``, ``buffer_size``, and ``backend`` will be ignored. We recommend that you port your code to rely directly on TorchCodec's decoder instead: https://docs.pytorch.org/torchcodec/stable/generated/torchcodec.decoders.AudioDecoder.html#torchcodec.decoders.AudioDecoder.\n",
      "  warnings.warn(\n",
      "c:\\Users\\norak\\SpeakSense\\venv\\Lib\\site-packages\\pyannote\\audio\\core\\io.py:85: UserWarning: torchaudio._backend.utils.info has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n",
      "  info = torchaudio.info(file[\"audio\"], backend=backend)\n",
      "c:\\Users\\norak\\SpeakSense\\venv\\Lib\\site-packages\\torchaudio\\_backend\\soundfile_backend.py:120: UserWarning: torchaudio._backend.common.AudioMetaData has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n",
      "  return AudioMetaData(\n",
      "c:\\Users\\norak\\SpeakSense\\venv\\Lib\\site-packages\\torchaudio\\_backend\\utils.py:213: UserWarning: In 2.9, this function's implementation will be changed to use torchaudio.load_with_torchcodec` under the hood. Some parameters like ``normalize``, ``format``, ``buffer_size``, and ``backend`` will be ignored. We recommend that you port your code to rely directly on TorchCodec's decoder instead: https://docs.pytorch.org/torchcodec/stable/generated/torchcodec.decoders.AudioDecoder.html#torchcodec.decoders.AudioDecoder.\n",
      "  warnings.warn(\n",
      "c:\\Users\\norak\\SpeakSense\\venv\\Lib\\site-packages\\pyannote\\audio\\core\\io.py:85: UserWarning: torchaudio._backend.utils.info has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n",
      "  info = torchaudio.info(file[\"audio\"], backend=backend)\n",
      "c:\\Users\\norak\\SpeakSense\\venv\\Lib\\site-packages\\torchaudio\\_backend\\soundfile_backend.py:120: UserWarning: torchaudio._backend.common.AudioMetaData has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n",
      "  return AudioMetaData(\n",
      "c:\\Users\\norak\\SpeakSense\\venv\\Lib\\site-packages\\torchaudio\\_backend\\utils.py:213: UserWarning: In 2.9, this function's implementation will be changed to use torchaudio.load_with_torchcodec` under the hood. Some parameters like ``normalize``, ``format``, ``buffer_size``, and ``backend`` will be ignored. We recommend that you port your code to rely directly on TorchCodec's decoder instead: https://docs.pytorch.org/torchcodec/stable/generated/torchcodec.decoders.AudioDecoder.html#torchcodec.decoders.AudioDecoder.\n",
      "  warnings.warn(\n",
      "c:\\Users\\norak\\SpeakSense\\venv\\Lib\\site-packages\\pyannote\\audio\\core\\io.py:85: UserWarning: torchaudio._backend.utils.info has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n",
      "  info = torchaudio.info(file[\"audio\"], backend=backend)\n",
      "c:\\Users\\norak\\SpeakSense\\venv\\Lib\\site-packages\\torchaudio\\_backend\\soundfile_backend.py:120: UserWarning: torchaudio._backend.common.AudioMetaData has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n",
      "  return AudioMetaData(\n",
      "c:\\Users\\norak\\SpeakSense\\venv\\Lib\\site-packages\\torchaudio\\_backend\\utils.py:213: UserWarning: In 2.9, this function's implementation will be changed to use torchaudio.load_with_torchcodec` under the hood. Some parameters like ``normalize``, ``format``, ``buffer_size``, and ``backend`` will be ignored. We recommend that you port your code to rely directly on TorchCodec's decoder instead: https://docs.pytorch.org/torchcodec/stable/generated/torchcodec.decoders.AudioDecoder.html#torchcodec.decoders.AudioDecoder.\n",
      "  warnings.warn(\n",
      "c:\\Users\\norak\\SpeakSense\\venv\\Lib\\site-packages\\pyannote\\audio\\core\\io.py:85: UserWarning: torchaudio._backend.utils.info has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n",
      "  info = torchaudio.info(file[\"audio\"], backend=backend)\n",
      "c:\\Users\\norak\\SpeakSense\\venv\\Lib\\site-packages\\torchaudio\\_backend\\soundfile_backend.py:120: UserWarning: torchaudio._backend.common.AudioMetaData has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n",
      "  return AudioMetaData(\n",
      "c:\\Users\\norak\\SpeakSense\\venv\\Lib\\site-packages\\torchaudio\\_backend\\utils.py:213: UserWarning: In 2.9, this function's implementation will be changed to use torchaudio.load_with_torchcodec` under the hood. Some parameters like ``normalize``, ``format``, ``buffer_size``, and ``backend`` will be ignored. We recommend that you port your code to rely directly on TorchCodec's decoder instead: https://docs.pytorch.org/torchcodec/stable/generated/torchcodec.decoders.AudioDecoder.html#torchcodec.decoders.AudioDecoder.\n",
      "  warnings.warn(\n",
      "c:\\Users\\norak\\SpeakSense\\venv\\Lib\\site-packages\\pyannote\\audio\\core\\io.py:85: UserWarning: torchaudio._backend.utils.info has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n",
      "  info = torchaudio.info(file[\"audio\"], backend=backend)\n",
      "c:\\Users\\norak\\SpeakSense\\venv\\Lib\\site-packages\\torchaudio\\_backend\\soundfile_backend.py:120: UserWarning: torchaudio._backend.common.AudioMetaData has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n",
      "  return AudioMetaData(\n",
      "c:\\Users\\norak\\SpeakSense\\venv\\Lib\\site-packages\\torchaudio\\_backend\\utils.py:213: UserWarning: In 2.9, this function's implementation will be changed to use torchaudio.load_with_torchcodec` under the hood. Some parameters like ``normalize``, ``format``, ``buffer_size``, and ``backend`` will be ignored. We recommend that you port your code to rely directly on TorchCodec's decoder instead: https://docs.pytorch.org/torchcodec/stable/generated/torchcodec.decoders.AudioDecoder.html#torchcodec.decoders.AudioDecoder.\n",
      "  warnings.warn(\n",
      "c:\\Users\\norak\\SpeakSense\\venv\\Lib\\site-packages\\pyannote\\audio\\core\\io.py:85: UserWarning: torchaudio._backend.utils.info has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n",
      "  info = torchaudio.info(file[\"audio\"], backend=backend)\n",
      "c:\\Users\\norak\\SpeakSense\\venv\\Lib\\site-packages\\torchaudio\\_backend\\soundfile_backend.py:120: UserWarning: torchaudio._backend.common.AudioMetaData has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n",
      "  return AudioMetaData(\n",
      "c:\\Users\\norak\\SpeakSense\\venv\\Lib\\site-packages\\torchaudio\\_backend\\utils.py:213: UserWarning: In 2.9, this function's implementation will be changed to use torchaudio.load_with_torchcodec` under the hood. Some parameters like ``normalize``, ``format``, ``buffer_size``, and ``backend`` will be ignored. We recommend that you port your code to rely directly on TorchCodec's decoder instead: https://docs.pytorch.org/torchcodec/stable/generated/torchcodec.decoders.AudioDecoder.html#torchcodec.decoders.AudioDecoder.\n",
      "  warnings.warn(\n",
      "c:\\Users\\norak\\SpeakSense\\venv\\Lib\\site-packages\\pyannote\\audio\\core\\io.py:85: UserWarning: torchaudio._backend.utils.info has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n",
      "  info = torchaudio.info(file[\"audio\"], backend=backend)\n",
      "c:\\Users\\norak\\SpeakSense\\venv\\Lib\\site-packages\\torchaudio\\_backend\\soundfile_backend.py:120: UserWarning: torchaudio._backend.common.AudioMetaData has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n",
      "  return AudioMetaData(\n",
      "c:\\Users\\norak\\SpeakSense\\venv\\Lib\\site-packages\\torchaudio\\_backend\\utils.py:213: UserWarning: In 2.9, this function's implementation will be changed to use torchaudio.load_with_torchcodec` under the hood. Some parameters like ``normalize``, ``format``, ``buffer_size``, and ``backend`` will be ignored. We recommend that you port your code to rely directly on TorchCodec's decoder instead: https://docs.pytorch.org/torchcodec/stable/generated/torchcodec.decoders.AudioDecoder.html#torchcodec.decoders.AudioDecoder.\n",
      "  warnings.warn(\n",
      "c:\\Users\\norak\\SpeakSense\\venv\\Lib\\site-packages\\pyannote\\audio\\core\\io.py:85: UserWarning: torchaudio._backend.utils.info has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n",
      "  info = torchaudio.info(file[\"audio\"], backend=backend)\n",
      "c:\\Users\\norak\\SpeakSense\\venv\\Lib\\site-packages\\torchaudio\\_backend\\soundfile_backend.py:120: UserWarning: torchaudio._backend.common.AudioMetaData has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n",
      "  return AudioMetaData(\n",
      "c:\\Users\\norak\\SpeakSense\\venv\\Lib\\site-packages\\torchaudio\\_backend\\utils.py:213: UserWarning: In 2.9, this function's implementation will be changed to use torchaudio.load_with_torchcodec` under the hood. Some parameters like ``normalize``, ``format``, ``buffer_size``, and ``backend`` will be ignored. We recommend that you port your code to rely directly on TorchCodec's decoder instead: https://docs.pytorch.org/torchcodec/stable/generated/torchcodec.decoders.AudioDecoder.html#torchcodec.decoders.AudioDecoder.\n",
      "  warnings.warn(\n",
      "c:\\Users\\norak\\SpeakSense\\venv\\Lib\\site-packages\\pyannote\\audio\\core\\io.py:85: UserWarning: torchaudio._backend.utils.info has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n",
      "  info = torchaudio.info(file[\"audio\"], backend=backend)\n",
      "c:\\Users\\norak\\SpeakSense\\venv\\Lib\\site-packages\\torchaudio\\_backend\\soundfile_backend.py:120: UserWarning: torchaudio._backend.common.AudioMetaData has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n",
      "  return AudioMetaData(\n",
      "c:\\Users\\norak\\SpeakSense\\venv\\Lib\\site-packages\\torchaudio\\_backend\\utils.py:213: UserWarning: In 2.9, this function's implementation will be changed to use torchaudio.load_with_torchcodec` under the hood. Some parameters like ``normalize``, ``format``, ``buffer_size``, and ``backend`` will be ignored. We recommend that you port your code to rely directly on TorchCodec's decoder instead: https://docs.pytorch.org/torchcodec/stable/generated/torchcodec.decoders.AudioDecoder.html#torchcodec.decoders.AudioDecoder.\n",
      "  warnings.warn(\n",
      "c:\\Users\\norak\\SpeakSense\\venv\\Lib\\site-packages\\pyannote\\audio\\core\\io.py:85: UserWarning: torchaudio._backend.utils.info has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n",
      "  info = torchaudio.info(file[\"audio\"], backend=backend)\n",
      "c:\\Users\\norak\\SpeakSense\\venv\\Lib\\site-packages\\torchaudio\\_backend\\soundfile_backend.py:120: UserWarning: torchaudio._backend.common.AudioMetaData has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n",
      "  return AudioMetaData(\n",
      "c:\\Users\\norak\\SpeakSense\\venv\\Lib\\site-packages\\torchaudio\\_backend\\utils.py:213: UserWarning: In 2.9, this function's implementation will be changed to use torchaudio.load_with_torchcodec` under the hood. Some parameters like ``normalize``, ``format``, ``buffer_size``, and ``backend`` will be ignored. We recommend that you port your code to rely directly on TorchCodec's decoder instead: https://docs.pytorch.org/torchcodec/stable/generated/torchcodec.decoders.AudioDecoder.html#torchcodec.decoders.AudioDecoder.\n",
      "  warnings.warn(\n",
      "c:\\Users\\norak\\SpeakSense\\venv\\Lib\\site-packages\\pyannote\\audio\\core\\io.py:85: UserWarning: torchaudio._backend.utils.info has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n",
      "  info = torchaudio.info(file[\"audio\"], backend=backend)\n",
      "c:\\Users\\norak\\SpeakSense\\venv\\Lib\\site-packages\\torchaudio\\_backend\\soundfile_backend.py:120: UserWarning: torchaudio._backend.common.AudioMetaData has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n",
      "  return AudioMetaData(\n",
      "c:\\Users\\norak\\SpeakSense\\venv\\Lib\\site-packages\\torchaudio\\_backend\\utils.py:213: UserWarning: In 2.9, this function's implementation will be changed to use torchaudio.load_with_torchcodec` under the hood. Some parameters like ``normalize``, ``format``, ``buffer_size``, and ``backend`` will be ignored. We recommend that you port your code to rely directly on TorchCodec's decoder instead: https://docs.pytorch.org/torchcodec/stable/generated/torchcodec.decoders.AudioDecoder.html#torchcodec.decoders.AudioDecoder.\n",
      "  warnings.warn(\n",
      "c:\\Users\\norak\\SpeakSense\\venv\\Lib\\site-packages\\pyannote\\audio\\core\\io.py:85: UserWarning: torchaudio._backend.utils.info has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n",
      "  info = torchaudio.info(file[\"audio\"], backend=backend)\n",
      "c:\\Users\\norak\\SpeakSense\\venv\\Lib\\site-packages\\torchaudio\\_backend\\soundfile_backend.py:120: UserWarning: torchaudio._backend.common.AudioMetaData has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n",
      "  return AudioMetaData(\n",
      "c:\\Users\\norak\\SpeakSense\\venv\\Lib\\site-packages\\torchaudio\\_backend\\utils.py:213: UserWarning: In 2.9, this function's implementation will be changed to use torchaudio.load_with_torchcodec` under the hood. Some parameters like ``normalize``, ``format``, ``buffer_size``, and ``backend`` will be ignored. We recommend that you port your code to rely directly on TorchCodec's decoder instead: https://docs.pytorch.org/torchcodec/stable/generated/torchcodec.decoders.AudioDecoder.html#torchcodec.decoders.AudioDecoder.\n",
      "  warnings.warn(\n",
      "c:\\Users\\norak\\SpeakSense\\venv\\Lib\\site-packages\\pyannote\\audio\\core\\io.py:85: UserWarning: torchaudio._backend.utils.info has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n",
      "  info = torchaudio.info(file[\"audio\"], backend=backend)\n",
      "c:\\Users\\norak\\SpeakSense\\venv\\Lib\\site-packages\\torchaudio\\_backend\\soundfile_backend.py:120: UserWarning: torchaudio._backend.common.AudioMetaData has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n",
      "  return AudioMetaData(\n",
      "c:\\Users\\norak\\SpeakSense\\venv\\Lib\\site-packages\\torchaudio\\_backend\\utils.py:213: UserWarning: In 2.9, this function's implementation will be changed to use torchaudio.load_with_torchcodec` under the hood. Some parameters like ``normalize``, ``format``, ``buffer_size``, and ``backend`` will be ignored. We recommend that you port your code to rely directly on TorchCodec's decoder instead: https://docs.pytorch.org/torchcodec/stable/generated/torchcodec.decoders.AudioDecoder.html#torchcodec.decoders.AudioDecoder.\n",
      "  warnings.warn(\n",
      "c:\\Users\\norak\\SpeakSense\\venv\\Lib\\site-packages\\pyannote\\audio\\core\\io.py:85: UserWarning: torchaudio._backend.utils.info has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n",
      "  info = torchaudio.info(file[\"audio\"], backend=backend)\n",
      "c:\\Users\\norak\\SpeakSense\\venv\\Lib\\site-packages\\torchaudio\\_backend\\soundfile_backend.py:120: UserWarning: torchaudio._backend.common.AudioMetaData has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n",
      "  return AudioMetaData(\n",
      "c:\\Users\\norak\\SpeakSense\\venv\\Lib\\site-packages\\torchaudio\\_backend\\utils.py:213: UserWarning: In 2.9, this function's implementation will be changed to use torchaudio.load_with_torchcodec` under the hood. Some parameters like ``normalize``, ``format``, ``buffer_size``, and ``backend`` will be ignored. We recommend that you port your code to rely directly on TorchCodec's decoder instead: https://docs.pytorch.org/torchcodec/stable/generated/torchcodec.decoders.AudioDecoder.html#torchcodec.decoders.AudioDecoder.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"TEST 3: WhisperX + Alignment + Diarization\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# We already have transcription and alignment from previous test\n",
    "# Just need to add diarization\n",
    "\n",
    "from pyannote.audio import Pipeline\n",
    "\n",
    "# TIME: Diarization\n",
    "print(\"\\n[1/2] Loading diarization model...\")\n",
    "start_diarize_load = time.time()\n",
    "diarize_model = Pipeline.from_pretrained(\n",
    "    \"pyannote/speaker-diarization-3.1\",\n",
    "    use_auth_token=HF_TOKEN\n",
    ")\n",
    "diarize_load_time = time.time() - start_diarize_load\n",
    "print(f\"Model loaded in {diarize_load_time:.2f} seconds\")\n",
    "\n",
    "print(\"\\n[2/2] Running speaker diarization...\")\n",
    "start_diarize = time.time()\n",
    "diarize_segments = diarize_model(audio_file)\n",
    "diarize_time = time.time() - start_diarize\n",
    "print(f\"Diarization complete in {diarize_time:.2f} seconds\")\n",
    "\n",
    "# Convert to DataFrame and assign speakers\n",
    "import pandas as pd\n",
    "diarize_list = []\n",
    "for turn, _, speaker in diarize_segments.itertracks(yield_label=True):\n",
    "    diarize_list.append({\n",
    "        'start': turn.start,\n",
    "        'end': turn.end,\n",
    "        'speaker': speaker\n",
    "    })\n",
    "diarize_df = pd.DataFrame(diarize_list)\n",
    "\n",
    "# Assign speakers to words\n",
    "full_result = whisperx.assign_word_speakers(diarize_df, whisperx_result)\n",
    "\n",
    "# Store results\n",
    "total_time = transcribe_time + align_time + diarize_time\n",
    "results['model'].append('WhisperX + Align + Diarization')\n",
    "results['transcription_time'].append(transcribe_time)\n",
    "results['alignment_time'].append(align_time)\n",
    "results['diarization_time'].append(diarize_time)\n",
    "results['total_time'].append(total_time)\n",
    "results['segments_found'].append(len(full_result['segments']))\n",
    "results['memory_usage'].append('N/A')\n",
    "\n",
    "print(f\"\\nResults:\")\n",
    "print(f\"  Transcription time: {transcribe_time:.2f}s\")\n",
    "print(f\"  Alignment time: {align_time:.2f}s\")\n",
    "print(f\"  Diarization time: {diarize_time:.2f}s\")\n",
    "print(f\"  Total time: {total_time:.2f}s\")\n",
    "print(f\"  Speakers found: {len(diarize_df['speaker'].unique())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4106ba21",
   "metadata": {},
   "source": [
    "## Step Five: Compare Results\n",
    "\n",
    "Let's visualize the performance differences."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
