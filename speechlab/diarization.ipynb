{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9837316",
   "metadata": {},
   "source": [
    "# Speech Diarization for Media Bias Analysis\n",
    "\n",
    "## Project Overview\n",
    "This notebook demonstrates speech diarization - the process of identifying \"who spoke when\" in an audio recording. We'll use this to analyze speaking time distribution in media, which can reveal potential biases and give context.\n",
    "\n",
    "## What is Diarization?\n",
    "- **Transcription** = converting speech to text\n",
    "- **Diarization** = identifying different speakers\n",
    "- **My Goal** = measure speaking time, interruptions, and speaking patterns to detect media bias\n",
    "\n",
    "## Steps:\n",
    "1. Load and transcribe audio with WhisperX\n",
    "2. Perform speaker diarization\n",
    "3. Align transcription with speaker labels\n",
    "4. Calculate speaking time statistics\n",
    "5. Analyze patterns for bias indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "257d374f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported\n",
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import whisperx  #Speech recognition & speaker diarization\n",
    "import torch    # Deep learning operations\n",
    "\n",
    "print(\"Libraries imported\")\n",
    "\n",
    "# Checks if GPU is available, else uses CPU\n",
    "print(f\"Using device: {'cuda' if torch.cuda.is_available() else 'cpu'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b522f0c",
   "metadata": {},
   "source": [
    "## Load the Speech Recognition Model\n",
    "\n",
    "<b>\"base\"</b> model is a good pick - fast enough for testing, accurate enough for real analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1c843fc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading WhisperX model...\n",
      "2025-11-03 21:52:35 - whisperx.asr - INFO - No language specified, language will be detected for each audio file (increases inference time)\n",
      "2025-11-03 21:52:35 - whisperx.vads.pyannote - INFO - Performing voice activity detection using Pyannote...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.5.5. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint c:\\Users\\norak\\SpeakSense\\venv\\Lib\\site-packages\\whisperx\\assets\\pytorch_model.bin`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model was trained with pyannote.audio 0.0.1, yours is 3.4.0. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
      "Model was trained with torch 1.10.0+cu102, yours is 2.8.0+cpu. Bad things might happen unless you revert torch to 1.x.\n",
      "Model loaded successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\norak\\SpeakSense\\venv\\Lib\\site-packages\\pyannote\\audio\\core\\io.py:212: UserWarning: torchaudio._backend.list_audio_backends has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n",
      "  torchaudio.list_audio_backends()\n"
     ]
    }
   ],
   "source": [
    "# Load the WhisperX model\n",
    "print(\"Loading WhisperX model...\")\n",
    "\n",
    "#Downloads & initializes the AI model\n",
    "model = whisperx.load_model(\"base\", device=\"cpu\", compute_type=\"int8\")\n",
    "\n",
    "print(\"Model loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba29157",
   "metadata": {},
   "source": [
    "## Transcribe Audio\n",
    "This converts the speech in our audio file to text with precise timestamps.\n",
    "\n",
    "**What happens:**\n",
    "1. Audio is loaded and converted to the right format (using ffmpeg)\n",
    "2. The AI model processes the audio in chunks\n",
    "3. Each chunk is transcribed to text\n",
    "4. Timestamps mark when each segment starts and ends\n",
    "\n",
    "**Output:** A dictionary containing segments of transcribed text with timing information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fc892f8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcribing: ../data/sampleaudio.wav\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-03 21:52:36 - whisperx.asr - WARNING - Audio is shorter than 30s, language detection may be inaccurate\n",
      "2025-11-03 21:52:37 - whisperx.asr - INFO - Detected language: en (0.99) in first 30s of audio\n",
      "Done! Found 1 segments\n"
     ]
    }
   ],
   "source": [
    "# Transcribe the audio file\n",
    "\n",
    "# Path to the audio file\n",
    "audio_file = \"../data/sampleaudio.wav\"\n",
    "\n",
    "print(f\"Transcribing: {audio_file}\")\n",
    "# Processes the audio and returns a dictionary - segments with text & timestamps & language info\n",
    "result = model.transcribe(audio_file)\n",
    "\n",
    "# Counts how many speech segments were found\n",
    "print(f\"Done! Found {len(result['segments'])} segments\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9e6879",
   "metadata": {},
   "source": [
    "## View Transcription Results\n",
    "Let's see what was said and when. Each segment shows the time range and the transcribed text.\n",
    "\n",
    "Each segment shows:\n",
    "- **Segment number** [1], [2], [3]...\n",
    "- **Time range** when it was spoken (e.g., 0.50s → 3.20s)\n",
    "- **Transcribed text** what was actually said"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9535f684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "TRANSCRIPTION RESULTS\n",
      "==================================================\n",
      "\n",
      "[1] 0.96s → 11.98s\n",
      "     Maybe I'm not good enough. Yes you are. Maybe I'm not. It's like, maybe I'm one of those people who is always gonna dream about doing stuff. You're not. You're gonna do it.\n"
     ]
    }
   ],
   "source": [
    "# Display the transcription with timestamps\n",
    "print(\"=\" * 50)\n",
    "print(\"TRANSCRIPTION RESULTS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Iterate through segments and print start time, end time, and text\n",
    "for i, segment in enumerate(result['segments'], 1):\n",
    "\n",
    "    start = segment['start'] \n",
    "    end = segment['end']\n",
    "    text = segment['text']\n",
    "    # Print timestamp to 2 decimal places\n",
    "    print(f\"\\n[{i}] {start:.2f}s → {end:.2f}s\")\n",
    "    print(f\"    {text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bea4e5e",
   "metadata": {},
   "source": [
    "## Perform Speaker Diarization\n",
    "Now we can start to identify WHO is speaking. \n",
    "\n",
    "Using pyannote.audio to cluster voices and label them as SPEAKER_00, SPEAKER_01, etc.\n",
    "\n",
    "First Import the Hugging Face Token from our .env file so we can access the pyannote diarization models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "42e9d274",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports to access token from .env file\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load variables from .env file into environment\n",
    "load_dotenv()\n",
    "HF_TOKEN = os.getenv(\"HF_TOKEN\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604a0d05",
   "metadata": {},
   "source": [
    "## Align Timestamps to Words\n",
    "\n",
    "Before diarization, we need word-level timestamps (not just phrase-level). This alignment model improves precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "81c24554",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aligning timestamps...\n",
      "Timestamps aligned successfully\n"
     ]
    }
   ],
   "source": [
    "print(\"Aligning timestamps...\")\n",
    "\n",
    "# Load the alignment model for English\n",
    "model_a, metadata = whisperx.load_align_model(language_code=\"en\", device=\"cpu\")\n",
    "\n",
    "# Align our transcription to get word-level timestamps\n",
    "# More accurate speaker assignment\n",
    "# We run our transcribed segments through this alignment model \n",
    "# alongside our original audio file for speaker alignment\n",
    "result = whisperx.align(result[\"segments\"], model_a, metadata, audio_file, device=\"cpu\")\n",
    "\n",
    "print(\"Timestamps aligned successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e13d4f1",
   "metadata": {},
   "source": [
    "## Load Speaker Diarization Model\n",
    "\n",
    "Now load the **diarization model** (pyannote.audio) which identifies different speakers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ed99e4a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading diarization model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\norak\\SpeakSense\\venv\\Lib\\site-packages\\pyannote\\audio\\core\\io.py:212: UserWarning: torchaudio._backend.list_audio_backends has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n",
      "  torchaudio.list_audio_backends()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diarization model loaded\n",
      "Running diarization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\norak\\SpeakSense\\venv\\Lib\\site-packages\\torchaudio\\_backend\\utils.py:213: UserWarning: In 2.9, this function's implementation will be changed to use torchaudio.load_with_torchcodec` under the hood. Some parameters like ``normalize``, ``format``, ``buffer_size``, and ``backend`` will be ignored. We recommend that you port your code to rely directly on TorchCodec's decoder instead: https://docs.pytorch.org/torchcodec/stable/generated/torchcodec.decoders.AudioDecoder.html#torchcodec.decoders.AudioDecoder.\n",
      "  warnings.warn(\n",
      "c:\\Users\\norak\\SpeakSense\\venv\\Lib\\site-packages\\pyannote\\audio\\models\\blocks\\pooling.py:104: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\native\\ReduceOps.cpp:1839.)\n",
      "  std = sequences.std(dim=-1, correction=1)\n",
      "c:\\Users\\norak\\SpeakSense\\venv\\Lib\\site-packages\\pyannote\\audio\\core\\io.py:85: UserWarning: torchaudio._backend.utils.info has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n",
      "  info = torchaudio.info(file[\"audio\"], backend=backend)\n",
      "c:\\Users\\norak\\SpeakSense\\venv\\Lib\\site-packages\\torchaudio\\_backend\\soundfile_backend.py:120: UserWarning: torchaudio._backend.common.AudioMetaData has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n",
      "  return AudioMetaData(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diarization complete\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading diarization model...\")\n",
    "from pyannote.audio import Pipeline\n",
    "\n",
    "# Load the pre-trained diarization model from Hugging Face\n",
    "# Pipeline.from_pretrained() downloads the model if not cached\n",
    "diarize_model = Pipeline.from_pretrained(\n",
    "    \"pyannote/speaker-diarization-3.1\",\n",
    "    use_auth_token=HF_TOKEN\n",
    ")\n",
    "\n",
    "print(\"Diarization model loaded\")\n",
    "\n",
    "print(\"Running diarization...\")\n",
    "\n",
    "# Analyzes the audio file to identify speakers and their speaking times\n",
    "# Clusters similar voices using machine learning\n",
    "diarize_segments = diarize_model(audio_file)\n",
    "print(\"Diarization complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a9094b",
   "metadata": {},
   "source": [
    "## View Diarization Results\n",
    "\n",
    "This shows **WHO** spoke **WHEN** (not what they said ... just yet).\n",
    "\n",
    "**Format:** `SPEAKER_XX: start_time → end_time`\n",
    "\n",
    "**What you'll see:**\n",
    "- SPEAKER_00, SPEAKER_01, etc. (standard labels assigned by the model)\n",
    "- Time ranges when each speaker was talking\n",
    "- Note: The same person gets the same label throughout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "39237d76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "DIARIZATION RESULTS\n",
      "==================================================\n",
      "\n",
      "SPEAKER_00: 0.96s → 2.16s\n",
      "\n",
      "SPEAKER_01: 2.22s → 2.24s\n",
      "\n",
      "SPEAKER_02: 2.24s → 3.05s\n",
      "\n",
      "SPEAKER_00: 3.64s → 4.44s\n",
      "\n",
      "SPEAKER_00: 4.77s → 5.31s\n",
      "\n",
      "SPEAKER_00: 5.89s → 9.65s\n",
      "\n",
      "SPEAKER_01: 10.21s → 12.01s\n"
     ]
    }
   ],
   "source": [
    "# Display diarization results\n",
    "print(\"=\" * 50)\n",
    "print(\"DIARIZATION RESULTS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "\n",
    "# itertracks() iterates through speaker segments\n",
    "# yield_label=True : includes the speaker label\n",
    "\n",
    "for turn, _, speaker in diarize_segments.itertracks(yield_label=True):\n",
    "    print(f\"\\n{speaker}: {turn.start:.2f}s → {turn.end:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55825dd",
   "metadata": {},
   "source": [
    "## Assign Speakers to Words\n",
    "\n",
    "Now we combine two pieces of information:\n",
    "- **Transcription** (what was said, from WhisperX)\n",
    "- **Diarization** (who was speaking when, from pyannote)\n",
    "\n",
    "**The challenge:** \n",
    "- pyannote gives us time ranges for speakers and WhisperX gives us time ranges for words\n",
    "- We need to match them up by finding overlaps\n",
    "\n",
    "**The solution:**\n",
    "Convert pyannote's format to a pandas DataFrame, then use WhisperX's `assign_word_speakers()` function to match speakers to words based on timestamp overlaps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b53cbfb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assigning speakers to transcription...\n",
      "Speakers assigned to words\n"
     ]
    }
   ],
   "source": [
    "# Assign speakers to words\n",
    "print(\"Assigning speakers to transcription...\")\n",
    "\n",
    "# Import pandas for DataFrame manipulation\n",
    "import pandas as pd\n",
    "\n",
    "# Convert pyannote format to pandas DataFrame\n",
    "# WhisperX expects diarization data as a DataFrame with columns:\n",
    "# 'start', 'end', 'speaker'\n",
    "\n",
    "# Extract speaker segments into a list of dictionaries\n",
    "diarize_list = []\n",
    "for turn, _, speaker in diarize_segments.itertracks(yield_label=True):\n",
    "    # Create a dictionary for each speaker segment\n",
    "    diarize_list.append({\n",
    "        'start': turn.start,\n",
    "        'end': turn.end,\n",
    "        'speaker': speaker\n",
    "    })\n",
    "\n",
    "# Convert list to DataFrame (table format)\n",
    "diarize_df = pd.DataFrame(diarize_list)\n",
    "\n",
    "# Assign speakers to words\n",
    "# whisperx.assign_word_speakers() matches speakers to words by:\n",
    "# - Comparing timestamps of speaker segments with word timestamps\n",
    "# - Finding overlaps to determine who was speaking each word\n",
    "\n",
    "# Takes in - the DataFrame we just created and the transcription result\n",
    "result = whisperx.assign_word_speakers(diarize_df, result)\n",
    "print(\"Speakers assigned to words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0ac7738f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "TRANSCRIPTION WITH SPEAKERS\n",
      "==================================================\n",
      "\n",
      "[SPEAKER_00] 0.96s → 2.14s\n",
      "     Maybe I'm not good enough.\n",
      "\n",
      "[SPEAKER_02] 2.34s → 3.03s\n",
      "    Yes you are.\n",
      "\n",
      "[SPEAKER_00] 3.73s → 4.43s\n",
      "    Maybe I'm not.\n",
      "\n",
      "[SPEAKER_00] 4.91s → 9.71s\n",
      "    It's like, maybe I'm one of those people who is always gonna dream about doing stuff.\n",
      "\n",
      "[SPEAKER_01] 10.33s → 10.87s\n",
      "    You're not.\n",
      "\n",
      "[SPEAKER_01] 11.15s → 12.00s\n",
      "    You're gonna do it.\n"
     ]
    }
   ],
   "source": [
    "# Display final results with speakers\n",
    "print(\"=\" * 50)\n",
    "print(\"TRANSCRIPTION WITH SPEAKERS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Loop through all segments\n",
    "for segment in result[\"segments\"]:\n",
    "    # Get speaker label - default to 'UNKNOWN' if not found\n",
    "    speaker = segment.get('speaker', 'UNKNOWN')\n",
    "\n",
    "    # Extract timing and text\n",
    "    start = segment['start']\n",
    "    end = segment['end']\n",
    "    text = segment['text']\n",
    "    # Print speaker label with timestamp\n",
    "    print(f\"\\n[{speaker}] {start:.2f}s → {end:.2f}s\")\n",
    "    print(f\"    {text}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
