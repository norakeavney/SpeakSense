{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9837316",
   "metadata": {},
   "source": [
    "# Speech Diarization for Media Bias Analysis\n",
    "\n",
    "## Project Overview\n",
    "This notebook demonstrates speech diarization - the process of identifying \"who spoke when\" in an audio recording. We'll use this to analyze speaking time distribution in media, which can reveal potential biases and give context.\n",
    "\n",
    "## What is Diarization?\n",
    "- **Transcription** = converting speech to text\n",
    "- **Diarization** = identifying different speakers\n",
    "- **My Goal** = measure speaking time, interruptions, and speaking patterns to detect media bias\n",
    "\n",
    "## Steps:\n",
    "1. Load and transcribe audio with WhisperX\n",
    "2. Perform speaker diarization\n",
    "3. Align transcription with speaker labels\n",
    "4. Calculate speaking time statistics\n",
    "5. Analyze patterns for bias indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257d374f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Libraries imported\n",
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import whisperx\n",
    "import torch\n",
    "\n",
    "print(\"Libraries imported\")\n",
    "print(f\"Using device: {'cuda' if torch.cuda.is_available() else 'cpu'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b522f0c",
   "metadata": {},
   "source": [
    "## Load the Speech Recognition Model\n",
    "\n",
    "<b>\"base\"</b> model is a good pick - fast enough for testing, accurate enough for real analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c843fc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading WhisperX model...\n",
      "2025-11-03 11:24:09 - whisperx.asr - INFO - No language specified, language will be detected for each audio file (increases inference time)\n",
      "2025-11-03 11:24:09 - whisperx.vads.pyannote - INFO - Performing voice activity detection using Pyannote...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.5.5. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint c:\\Users\\norak\\SpeakSense\\venv\\Lib\\site-packages\\whisperx\\assets\\pytorch_model.bin`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model was trained with pyannote.audio 0.0.1, yours is 3.4.0. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
      "Model was trained with torch 1.10.0+cu102, yours is 2.8.0+cpu. Bad things might happen unless you revert torch to 1.x.\n",
      "Model loaded successfully\n"
     ]
    }
   ],
   "source": [
    "# Load the WhisperX model\n",
    "print(\"Loading WhisperX model...\")\n",
    "model = whisperx.load_model(\"base\", device=\"cpu\", compute_type=\"int8\")\n",
    "print(\"Model loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba29157",
   "metadata": {},
   "source": [
    "## Transcribe Audio\n",
    "This converts the speech in our audio file to text with precise timestamps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc892f8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcribing: ../data/sampleaudio.wav\n",
      "2025-11-03 11:24:11 - whisperx.asr - WARNING - Audio is shorter than 30s, language detection may be inaccurate\n",
      "2025-11-03 11:24:12 - whisperx.asr - INFO - Detected language: en (0.99) in first 30s of audio\n",
      "Done! Found 1 segments\n"
     ]
    }
   ],
   "source": [
    "# Transcribe the audio file\n",
    "audio_file = \"../data/sampleaudio.wav\"\n",
    "print(f\"Transcribing: {audio_file}\")\n",
    "result = model.transcribe(audio_file)\n",
    "print(f\"Done! Found {len(result['segments'])} segments\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9e6879",
   "metadata": {},
   "source": [
    "## View Transcription Results\n",
    "Let's see what was said and when. Each segment shows the time range and the transcribed text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9535f684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "TRANSCRIPTION RESULTS\n",
      "==================================================\n",
      "\n",
      "[1] 0.96s → 11.98s\n",
      "     Maybe I'm not good enough. Yes you are. Maybe I'm not. It's like, maybe I'm one of those people who is always gonna dream about doing stuff. You're not. You're gonna do it.\n"
     ]
    }
   ],
   "source": [
    "# Display the transcription with timestamps\n",
    "print(\"=\" * 50)\n",
    "print(\"TRANSCRIPTION RESULTS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Iterate through segments and print start time, end time, and text\n",
    "for i, segment in enumerate(result['segments'], 1):\n",
    "    start = segment['start']\n",
    "    end = segment['end']\n",
    "    text = segment['text']\n",
    "    print(f\"\\n[{i}] {start:.2f}s → {end:.2f}s\")\n",
    "    print(f\"    {text}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
